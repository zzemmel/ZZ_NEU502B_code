{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b83a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded74b8c-37cd-43a8-b006-e66d69509fb9",
   "metadata": {},
   "source": [
    "Subject matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f2f8c-9795-498f-a53c-3f08021b3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Subject 2...\n",
      "Found 20 questions and 22 subject responses for Subject 2\n",
      "  Unable to match question nan: ...\n",
      "  Unable to match question nan: ...\n",
      "Created /Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files/subject2_matched.csv\n",
      "Summary: 20 questions matched, 2 questions unmatched\n",
      "\n",
      "Analyzing Subject 4...\n",
      "Found 20 questions and 22 subject responses for Subject 4\n",
      "  Unable to match question nan: ...\n",
      "  Unable to match question nan: ...\n",
      "Created /Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files/subject4_matched.csv\n",
      "Summary: 20 questions matched, 2 questions unmatched\n",
      "\n",
      "Analyzing Subject 5...\n",
      "Found 20 questions and 22 subject responses for Subject 5\n",
      "  Unable to match question nan: ...\n",
      "  Unable to match question nan: ...\n",
      "Created /Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files/subject5_matched.csv\n",
      "Summary: 20 questions matched, 2 questions unmatched\n",
      "\n",
      "subjects processed\n"
     ]
    }
   ],
   "source": [
    "# base path\n",
    "base_path = '/Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files' # change to your path\n",
    "\n",
    "# question versions and subject mappings\n",
    "question_files = {\n",
    "    2: 'fmri_questions_V1.csv',\n",
    "    4: 'fmri_questions_V2.csv',\n",
    "    5: 'fmri_questions_V3.csv'  \n",
    "}\n",
    "\n",
    "# subject2 - V1\n",
    "# subject4 - V2\n",
    "# subject5 - V3\n",
    "\n",
    "\n",
    "subject_files = {\n",
    "    2: 'subject2_question_looper.csv',\n",
    "    4: 'subject4_question_looper.csv',\n",
    "    5: 'subject5_question_looper.csv'\n",
    "}\n",
    "\n",
    "# answer key - necessary for matching because we didn't get questions labeled with \"math\" or \"language\" in our response metadata\n",
    "answers_key = {\n",
    "    \"What does a rose\": 1,\n",
    "    \"A gardener has found\": 2,\n",
    "    \"What does this saying\": 1,\n",
    "    \"A satellite completes\": 2,\n",
    "    \"Which next sentence best fits after The music\": 3,\n",
    "    \"An orchestra sells\": 2,\n",
    "    \"During a class discussion\": 2,\n",
    "    \"Emma's class started\": 4,\n",
    "    \"As Liam walked\": 1,\n",
    "    \"Liam usually walks\": 2,\n",
    "    \"It is a truth universally\": 4,\n",
    "    \"A soon-to-be husband\": 1,\n",
    "    \"James arrived at the café a few minutes early for his meeting. He ordered a coffee and sat\": 2,\n",
    "    \"James arrived at the café a few minutes early for his meeting. He ordered a coffee and one\": 3,\n",
    "    \"Which next sentence best fits after The newborn\": 3,\n",
    "    \"A small zoo has\": 1,\n",
    "    \"When the museum tour began, Daniel stayed\": 2,\n",
    "    \"When the museum tour began, Daniel walked\": 4,\n",
    "    \"What does the line Once\": 1,\n",
    "    \"A tired poet writes\": 2\n",
    "}\n",
    "\n",
    "# normalize question text for matching\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # convert to string\n",
    "    text = str(text)\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[\"\\'\\.,\\?]', '', text)\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# finds the correct answer for a question\n",
    "def find_answer(question_text):\n",
    "    if pd.isna(question_text) or not question_text:\n",
    "        return None\n",
    "        \n",
    "    normalized_question = normalize_text(question_text)\n",
    "    \n",
    "    # tries matching with the beginning of each key phrase\n",
    "    for key_start, answer in answers_key.items():\n",
    "        normalized_key = normalize_text(key_start)\n",
    "        if normalized_key and normalized_question.startswith(normalized_key):\n",
    "            return answer\n",
    "    \n",
    "    # if no exact start match, try a more flexible match\n",
    "    for key_start, answer in answers_key.items():\n",
    "        normalized_key = normalize_text(key_start)\n",
    "        if normalized_key and normalized_key in normalized_question:\n",
    "            return answer\n",
    "    \n",
    "    # look for distinctive phrases\n",
    "    if \"rose by any other name\" in normalized_question:\n",
    "        return 1\n",
    "    if \"four roses\" in normalized_question:\n",
    "        return 2\n",
    "    if \"satellite\" in normalized_question and \"orbit\" in normalized_question:\n",
    "        return 2\n",
    "    if \"music soared\" in normalized_question:\n",
    "        return 3\n",
    "    if \"orchestra\" in normalized_question and \"tickets\" in normalized_question:\n",
    "        return 2\n",
    "    if \"emma raised\" in normalized_question:\n",
    "        return 2\n",
    "    if \"emma\" in normalized_question and \"class started\" in normalized_question:\n",
    "        return 4\n",
    "    if \"liam\" in normalized_question and \"rain\" in normalized_question:\n",
    "        return 1\n",
    "    if \"liam\" in normalized_question and \"walks\" in normalized_question and \"pace\" in normalized_question:\n",
    "        return 2\n",
    "    if \"truth universally acknowledged\" in normalized_question:\n",
    "        return 4\n",
    "    if \"husband and wife\" in normalized_question:\n",
    "        return 1\n",
    "    if \"james\" in normalized_question and \"window\" in normalized_question:\n",
    "        return 2\n",
    "    if \"james\" in normalized_question and \"croissant\" in normalized_question:\n",
    "        return 3\n",
    "    if \"newborn animals\" in normalized_question:\n",
    "        return 3\n",
    "    if \"zoo\" in normalized_question and \"mammals\" in normalized_question:\n",
    "        return 1\n",
    "    if \"daniel stayed\" in normalized_question and \"museum\" in normalized_question:\n",
    "        return 2\n",
    "    if \"daniel walked\" in normalized_question and \"museum\" in normalized_question:\n",
    "        return 4\n",
    "    if \"midnight dreary\" in normalized_question:\n",
    "        return 1\n",
    "    if \"tired poet\" in normalized_question:\n",
    "        return 2\n",
    "    \n",
    "    print(f\"WARNING: No answer found for question: {question_text[:50]}...\")\n",
    "    return None\n",
    "\n",
    "# for each subject\n",
    "for subject_num in [2, 4, 5]:\n",
    "    print(f\"\\nAnalyzing Subject {subject_num}...\")\n",
    "    \n",
    "    # read the question file\n",
    "    question_file_path = os.path.join(base_path, question_files[subject_num])\n",
    "    questions_df = pd.read_csv(question_file_path)\n",
    "    \n",
    "    # read the subject response file\n",
    "    subject_file_path = os.path.join(base_path, subject_files[subject_num])\n",
    "    subject_df = pd.read_csv(subject_file_path)\n",
    "    \n",
    "    print(f\"Found {len(questions_df)} questions and {len(subject_df)} subject responses for Subject {subject_num}\")\n",
    "    \n",
    "    # create a dictionary to look up questions by their number\n",
    "    question_lookup = dict(zip(questions_df['Number'], questions_df['Question']))\n",
    "    \n",
    "    # create a new dataframe for the matched data\n",
    "    matched_data = []\n",
    "    \n",
    "    # keep track of matched/unmatched questions\n",
    "    matched_count = 0\n",
    "    unmatched_count = 0\n",
    "    \n",
    "    # process each row in the subject file\n",
    "    for _, row in subject_df.iterrows():\n",
    "        question_number = row['Number']\n",
    "        question_text = question_lookup.get(question_number, \"\")\n",
    "        \n",
    "        # get correct answer for this question\n",
    "        correct_answer = find_answer(question_text)\n",
    "        \n",
    "        if correct_answer is not None:\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "            print(f\"  Unable to match question {question_number}: {question_text[:50]}...\")\n",
    "        \n",
    "        # get subject's response\n",
    "        subject_response = row.get('key_resp.keys', None)\n",
    "        \n",
    "        # determine if answer was correct\n",
    "        if pd.notna(subject_response) and correct_answer is not None:\n",
    "            # extract numeric value from response (handles cases like '[1]' or '1')\n",
    "            if isinstance(subject_response, (int, float)):\n",
    "                subject_response_int = int(subject_response)\n",
    "            elif isinstance(subject_response, str):\n",
    "                # Extract digits from string responses\n",
    "                match = re.search(r'\\d+', subject_response)\n",
    "                if match:\n",
    "                    subject_response_int = int(match.group())\n",
    "                else:\n",
    "                    subject_response_int = None\n",
    "            else:\n",
    "                subject_response_int = None\n",
    "                \n",
    "            # compare with correct answer\n",
    "            if subject_response_int is not None:\n",
    "                is_correct = 1 if subject_response_int == correct_answer else 0\n",
    "            else:\n",
    "                is_correct = None\n",
    "        else:\n",
    "            is_correct = None  # Not answered or no correct answer available\n",
    "        \n",
    "        matched_data.append({\n",
    "            'question_number': question_number,\n",
    "            'question_text': question_text,\n",
    "            'question_started': row.get('Question_text.started', None),\n",
    "            'question_ended': row.get('trial.stopped', None),\n",
    "            'response_time': row.get('key_resp.rt', None),\n",
    "            'subject_response': subject_response,\n",
    "            'correct_answer': correct_answer,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "    \n",
    "    # create the matched dataframe\n",
    "    matched_df = pd.DataFrame(matched_data)\n",
    "    \n",
    "    # save as CSV\n",
    "    output_file = os.path.join(base_path, f\"subject{subject_num}_matched.csv\")\n",
    "    matched_df.to_csv(output_file, index=False)\n",
    "    print(f\"Created {output_file}\")\n",
    "    print(f\"Summary: {matched_count} questions matched, {unmatched_count} questions unmatched\")\n",
    "\n",
    "print(\"\\nsubjects processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e08475-11b2-444d-af59-b012a29cc391",
   "metadata": {},
   "source": [
    "Correct vs. Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfd1fb-9553-47e6-a837-bde511d12520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for Subject 2...\n",
      "Loading data for Subject 4...\n",
      "Loading data for Subject 5...\n",
      "Combined data has 66 rows\n",
      "\n",
      "Analyzing Subject 2...\n",
      "\n",
      "Analyzing Subject 4...\n",
      "\n",
      "Analyzing Subject 5...\n",
      "\n",
      "Analyzing combined data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/563011839.py:32: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n",
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/563011839.py:32: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n",
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/563011839.py:32: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n",
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/563011839.py:32: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of T-test Results:\n",
      "--------------------------------------------------------------------------------\n",
      "Subject    Correct RT           Incorrect RT         t-stat     p-value    Significant?\n",
      "--------------------------------------------------------------------------------\n",
      "Subject 2  19.589 ± 5.881       19.312 ± 9.647       0.067      0.9479     No          \n",
      "Subject 4  18.645 ± 7.736       17.031 ± 8.658       0.302      0.7849     No          \n",
      "Subject 5  16.810 ± 7.664       19.055 ± 2.105       -0.842     0.4273     No          \n",
      "combined   18.289 ± 7.211       18.698 ± 8.113       -0.156     0.8777     No          \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Plots saved to: /Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files/reaction_time_analysis.pdf\n"
     ]
    }
   ],
   "source": [
    "# pdf to save all plots\n",
    "pdf_path = os.path.join(base_path, 'reaction_time_analysis.pdf')\n",
    "pdf = PdfPages(pdf_path)\n",
    "\n",
    "# create function to create boxplots and perform t-tests\n",
    "def analyze_reaction_times(data, subject_id=None):\n",
    "    # only include rows with valid reaction times and is_correct values\n",
    "    filtered_data = data.dropna(subset=['response_time', 'is_correct'])\n",
    "    \n",
    "    # separate correct and incorrect answers\n",
    "    correct_rt = filtered_data[filtered_data['is_correct'] == 1]['response_time']\n",
    "    incorrect_rt = filtered_data[filtered_data['is_correct'] == 0]['response_time']\n",
    "    \n",
    "    # check if we have enough data for both categories\n",
    "    if len(correct_rt) < 2 or len(incorrect_rt) < 2:\n",
    "        print(f\"Not enough data for analysis for {'Subject ' + str(subject_id) if subject_id else 'Combined'}\")\n",
    "        return None\n",
    "    \n",
    "    # perform t-test\n",
    "    t_stat, p_value = stats.ttest_ind(correct_rt, incorrect_rt, equal_var=False)\n",
    "    \n",
    "    # create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # set boxplot elements\n",
    "    boxplot = ax.boxplot(\n",
    "        [correct_rt, incorrect_rt],\n",
    "        labels=['Correct Answers', 'Incorrect Answers'],\n",
    "        patch_artist=True,\n",
    "        zorder=5, widths=0.5  # Lower zorder than the data points\n",
    "    )\n",
    "    \n",
    "\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.set_ylim(0, y_max)\n",
    "    \n",
    "\n",
    "    colors = ['lightgreen', 'darkorange']\n",
    "    for patch, color in zip(boxplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        \n",
    "\n",
    "    for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "        for item in boxplot[element]:\n",
    "            item.set(color='black')\n",
    "    \n",
    "    # plot actual data points on the chart\n",
    "    for i, (data_points, color, marker) in enumerate(zip(\n",
    "        [correct_rt, incorrect_rt], \n",
    "        ['black', 'black'],\n",
    "        ['o', 'o'] \n",
    "    )):\n",
    "        # generate random x values for jitter\n",
    "        x = np.random.normal(i+1, 0.04, size=len(data_points))\n",
    "\n",
    "        ax.scatter(x, data_points, alpha=1, s=25, color=color, \n",
    "                   marker=marker, edgecolor='black', linewidth=0.5, zorder=10)\n",
    "    \n",
    "    if subject_id:\n",
    "        ax.set_title(f'Subject {subject_id}: Reaction Time Comparison - Correct vs. Incorrect Answers', color='black')\n",
    "    else:\n",
    "        ax.set_title(f'All Subjects: Reaction Time Comparison - Correct vs. Incorrect Answers', color='black')\n",
    "    \n",
    "    ax.set_ylabel('Reaction Time (s)', fontsize=16, color='black')\n",
    "    \n",
    "    stats_text = (\n",
    "        f'Correct (n={len(correct_rt)}):\\n'\n",
    "        f'Mean = {correct_rt.mean():.3f}s\\n'\n",
    "        f'SD = {correct_rt.std():.3f}s\\n\\n'\n",
    "        f'Incorrect (n={len(incorrect_rt)}):\\n'\n",
    "        f'Mean = {incorrect_rt.mean():.3f}s\\n'\n",
    "        f'SD = {incorrect_rt.std():.3f}s\\n\\n'\n",
    "        f't-test: p = {p_value:.4f}'\n",
    "    )\n",
    "    \n",
    "    ax.text(0.5, 0.98, stats_text,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
    "            fontsize=13, zorder=15)\n",
    "    \n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.3, color='black')\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='black', labelsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    pdf.savefig(fig)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'correct_mean': correct_rt.mean(),\n",
    "        'correct_std': correct_rt.std(),\n",
    "        'incorrect_mean': incorrect_rt.mean(),\n",
    "        'incorrect_std': incorrect_rt.std(),\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'n_correct': len(correct_rt),\n",
    "        'n_incorrect': len(incorrect_rt)\n",
    "    }\n",
    "\n",
    "# load data for each subject\n",
    "subject_data = {}\n",
    "all_data = []\n",
    "\n",
    "for subject_num in [2, 4, 5]:\n",
    "    file_path = os.path.join(base_path, f\"subject{subject_num}_matched.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data for Subject {subject_num}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # store data for individual subject analysis\n",
    "        subject_data[subject_num] = df\n",
    "        \n",
    "        # add subject identifier and append to combined data\n",
    "        df['subject'] = subject_num\n",
    "        all_data.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File not found for Subject {subject_num}\")\n",
    "\n",
    "# combine all data\n",
    "if all_data:\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Combined data has {len(combined_data)} rows\")\n",
    "else:\n",
    "    print(\"No data found for any subject\")\n",
    "    combined_data = None\n",
    "\n",
    "# analyze each subject individually\n",
    "results = {}\n",
    "for subject_num, data in subject_data.items():\n",
    "    print(f\"\\nAnalyzing Subject {subject_num}...\")\n",
    "    results[subject_num] = analyze_reaction_times(data, subject_num)\n",
    "\n",
    "# analyze combined data\n",
    "if combined_data is not None:\n",
    "    print(\"\\nAnalyzing combined data...\")\n",
    "    results['combined'] = analyze_reaction_times(combined_data)\n",
    "\n",
    "pdf.close()\n",
    "\n",
    "print(\"\\nSummary of T-test Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Subject':<10} {'Correct RT':<20} {'Incorrect RT':<20} {'t-stat':<10} {'p-value':<10} {'Significant?':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for subject, result in results.items():\n",
    "    if result:\n",
    "        subject_label = subject if subject == 'combined' else f\"Subject {subject}\"\n",
    "        correct_rt = f\"{result['correct_mean']:.3f} ± {result['correct_std']:.3f}\"\n",
    "        incorrect_rt = f\"{result['incorrect_mean']:.3f} ± {result['incorrect_std']:.3f}\"\n",
    "        significant = \"Yes (p<0.05)\" if result['p_value'] < 0.05 else \"No\"\n",
    "        \n",
    "        print(f\"{subject_label:<10} {correct_rt:<20} {incorrect_rt:<20} {result['t_stat']:<10.3f} {result['p_value']:<10.4f} {significant:<12}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nPlots saved to: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a7c19-4285-4997-b558-516d0d6b0a62",
   "metadata": {},
   "source": [
    "Math vs. Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74798-7fd5-40df-815d-a0993ba52ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Math vs. Language Analysis...\n",
      "Loading data for Subject 2...\n",
      "Loading data for Subject 4...\n",
      "Loading data for Subject 5...\n",
      "Combined data has 66 rows\n",
      "\n",
      "=== Analyzing Math vs. Language Question Performance ===\n",
      "\n",
      "Analyzing Subject 2...\n",
      "  Analyzing accuracy for Subject 2...\n",
      "  Analyzing reaction times for Subject 2...\n",
      "\n",
      "Analyzing Subject 4...\n",
      "  Analyzing accuracy for Subject 4...\n",
      "  Analyzing reaction times for Subject 4...\n",
      "\n",
      "Analyzing Subject 5...\n",
      "  Analyzing accuracy for Subject 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/3539238805.py:223: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n",
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/3539238805.py:223: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n",
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/3539238805.py:223: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzing reaction times for Subject 5...\n",
      "\n",
      "Analyzing combined data (all subjects)...\n",
      "\n",
      "Summary of Math vs. Language Accuracy Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subject    Math Acc.  Lang Acc.  Math n   Lang n   Test            p-value    Significant?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subject 2  0.714      0.250      7        4        Fisher's exact test 0.2424     No          \n",
      "Subject 4  0.889      0.833      9        6        Fisher's exact test 1.0000     No          \n",
      "Subject 5  0.875      0.667      8        3        Fisher's exact test 0.4909     No          \n",
      "All        0.833      0.615      24       13       Fisher's exact test 0.2293     No          \n",
      "\n",
      "Summary of Math vs. Language Reaction Time Analysis:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subject    Math RT         Lang RT         Math n   Lang n   t-stat     p-value    Significant?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Subject 2  19.924 ± 6.126  21.904 ± 5.475  7        4        -0.491     0.6391     No          \n",
      "Subject 4  23.375 ± 5.373  15.713 ± 6.406  9        6        2.229      0.0520     No          \n",
      "Subject 5  15.082 ± 7.367  19.496 ± 5.249  8        3        -0.951     0.3899     No          \n",
      "All        19.604 ± 7.214  18.491 ± 6.473  24       13       0.464      0.6464     No          \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "All plots saved to: /Users/zachary/Desktop/Princeton/2024-25/NEU 502B/neu502b-2025/fMRI Experiment/Behavior files/math_language_analysis.pdf\n",
      "Math vs. Language Analysis complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/53wyx_3n7wj89ttqjws8msdc0000gn/T/ipykernel_69317/3539238805.py:223: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  boxplot = ax.boxplot(\n"
     ]
    }
   ],
   "source": [
    "# pdf to save plots\n",
    "pdf_path = os.path.join(base_path, 'math_language_analysis.pdf')\n",
    "pdf = PdfPages(pdf_path)\n",
    "\n",
    "math_questions = [\n",
    "    \"A gardener has found\",\n",
    "    \"A satellite completes\",\n",
    "    \"An orchestra sells\",\n",
    "    \"Emma's class started\",\n",
    "    \"Liam usually walks\",\n",
    "    \"A soon-to-be husband\",\n",
    "    \"James arrived at the café a few minutes early for his meeting. He ordered a coffee and one\",\n",
    "    \"A small zoo has\",\n",
    "    \"When the museum tour began, Daniel walked\",\n",
    "    \"A tired poet writes\"\n",
    "]\n",
    "\n",
    "language_questions = [\n",
    "    \"What does a rose\",\n",
    "    \"What does this saying\",\n",
    "    \"Which next sentence best fits after The music\",\n",
    "    \"During a class discussion\",\n",
    "    \"As Liam walked\",\n",
    "    \"It is a truth universally\",\n",
    "    \"James arrived at the café a few minutes early for his meeting. He ordered a coffee and sat\",\n",
    "    \"Which next sentence best fits after The newborn\",\n",
    "    \"When the museum tour began, Daniel stayed\",\n",
    "    \"What does the line Once\"\n",
    "]\n",
    "\n",
    "# determine question type\n",
    "def get_question_type(question_text):\n",
    "    if pd.isna(question_text) or not isinstance(question_text, str):\n",
    "        return None\n",
    "    \n",
    "    # check if it's a math question\n",
    "    for math_start in math_questions:\n",
    "        if math_start in question_text:\n",
    "            return \"math\"\n",
    "    \n",
    "    # check if it's a language question\n",
    "    for lang_start in language_questions:\n",
    "        if lang_start in question_text:\n",
    "            return \"language\"\n",
    "    \n",
    "    # if no match is found after checking both\n",
    "    return None\n",
    "\n",
    "#  analyze and plot accuracy comparison\n",
    "def plot_accuracy_comparison(data, subject_id=None):\n",
    "    \"\"\"\"\"\n",
    "    creates a bar plot comparing math vs language accuracy\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    data['question_type'] = data['question_text'].apply(get_question_type)\n",
    "    \n",
    "    # filter to only include rows with valid question_type and is_correct\n",
    "    filtered_data = data.dropna(subset=['question_type', 'is_correct'])\n",
    "    \n",
    "    # get math and language subsets\n",
    "    math_data = filtered_data[filtered_data['question_type'] == 'math']\n",
    "    language_data = filtered_data[filtered_data['question_type'] == 'language']\n",
    "    \n",
    "    # calculate accuracy for each type\n",
    "    math_accuracy = math_data['is_correct'].mean() if len(math_data) > 0 else 0\n",
    "    language_accuracy = language_data['is_correct'].mean() if len(language_data) > 0 else 0\n",
    "    \n",
    "    # counts\n",
    "    math_count = len(math_data)\n",
    "    language_count = len(language_data)\n",
    "    \n",
    "    # correct counts\n",
    "    math_correct = math_data['is_correct'].sum()\n",
    "    language_correct = language_data['is_correct'].sum()\n",
    "    \n",
    "    if math_count > 0 and language_count > 0:\n",
    "        # for proportions, chi-square test or Fisher's exact test\n",
    "        # create contingency table\n",
    "        contingency = np.array([\n",
    "            [math_correct, math_count - math_correct],\n",
    "            [language_correct, language_count - language_correct]\n",
    "        ])\n",
    "        \n",
    "        # use Fisher's exact test for small samples\n",
    "        if np.min(contingency) < 5:\n",
    "            _, p_value = stats.fisher_exact(contingency)\n",
    "            test_name = \"Fisher's exact test\"\n",
    "        else:\n",
    "            _, p_value, _, _ = stats.chi2_contingency(contingency)\n",
    "            test_name = \"Chi-square test\"\n",
    "    else:\n",
    "        p_value = None\n",
    "        test_name = \"Not enough data\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    bar_positions = [1, 2]\n",
    "    bar_width = 0.6\n",
    "    \n",
    "    bars = ax.bar(bar_positions, \n",
    "                  [math_accuracy, language_accuracy], \n",
    "                  width=bar_width,\n",
    "                  color=['lightskyblue', 'gold'],\n",
    "                  edgecolor='black',\n",
    "                  linewidth=1.5,\n",
    "                  zorder=3)\n",
    "    \n",
    "    # standard errors for error bars\n",
    "    math_se = np.sqrt(math_accuracy * (1 - math_accuracy) / math_count) if math_count > 0 else 0\n",
    "    lang_se = np.sqrt(language_accuracy * (1 - language_accuracy) / language_count) if language_count > 0 else 0\n",
    "    \n",
    "    ax.errorbar(bar_positions, \n",
    "                [math_accuracy, language_accuracy],\n",
    "                yerr=[math_se, lang_se],\n",
    "                fmt='none',\n",
    "                ecolor='black',\n",
    "                capsize=5,\n",
    "                elinewidth=1.5,\n",
    "                capthick=1.5,\n",
    "                zorder=10)\n",
    "    \n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    ax.set_ylabel('Proportion Correct', fontsize=15, color='black')\n",
    "    \n",
    "    if subject_id:\n",
    "        ax.set_title(f'Subject {subject_id}: Math vs. Language Accuracy', fontsize=14, color='black')\n",
    "    else:\n",
    "        ax.set_title(f'All Subjects: Math vs. Language Accuracy', fontsize=14, color='black')\n",
    "    \n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels(['Math', 'Language'], fontsize=14, color='black')\n",
    "    \n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.3, color='black')\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='black',labelsize=14)\n",
    "    \n",
    "    stats_text = (\n",
    "        f'Math (n={math_count}):\\n'\n",
    "        f'Accuracy = {math_accuracy:.3f}\\n\\n'\n",
    "        f'Language (n={language_count}):\\n'\n",
    "        f'Accuracy = {language_accuracy:.3f}\\n\\n'\n",
    "        f'{test_name}:\\np = {p_value:.4f}' if p_value is not None else 'Not enough data for statistical test'\n",
    "    )\n",
    "    \n",
    "    ax.text(0.5, 0.98, stats_text,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
    "            fontsize=13, zorder=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'math_accuracy': math_accuracy,\n",
    "        'language_accuracy': language_accuracy,\n",
    "        'math_count': math_count,\n",
    "        'language_count': language_count,\n",
    "        'p_value': p_value,\n",
    "        'test_name': test_name\n",
    "    }\n",
    "\n",
    "# function to plot reaction time comparison\n",
    "def plot_rt_comparison(data, subject_id=None):\n",
    "    \"\"\"\"\"\n",
    "    creates boxplot comparing math vs language reaction times\n",
    "    \"\"\"\"\"\n",
    "    # add question type to data\n",
    "    data['question_type'] = data['question_text'].apply(get_question_type)\n",
    "    \n",
    "    # filter to only include rows with valid question_type and response_time\n",
    "    filtered_data = data.dropna(subset=['question_type', 'response_time'])\n",
    "    \n",
    "    # math and language subsets\n",
    "    math_data = filtered_data[filtered_data['question_type'] == 'math']\n",
    "    language_data = filtered_data[filtered_data['question_type'] == 'language']\n",
    "    \n",
    "    # reaction times\n",
    "    math_rt = math_data['response_time'].values if len(math_data) > 0 else np.array([])\n",
    "    language_rt = language_data['response_time'].values if len(language_data) > 0 else np.array([])\n",
    "    \n",
    "    # perform t-test\n",
    "    if len(math_rt) > 1 and len(language_rt) > 1:\n",
    "        t_stat, p_value = stats.ttest_ind(math_rt, language_rt, equal_var=False)\n",
    "    else:\n",
    "        t_stat = None\n",
    "        p_value = None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    boxplot = ax.boxplot(\n",
    "        [math_rt, language_rt],\n",
    "        labels=['Math', 'Language'],\n",
    "        patch_artist=True,\n",
    "        zorder=5, widths=0.5\n",
    "    )\n",
    "    \n",
    "    colors = ['lightskyblue', 'gold']\n",
    "    for patch, color in zip(boxplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "        for item in boxplot[element]:\n",
    "            item.set(color='black')\n",
    "    \n",
    "    for i, data_points in enumerate([math_rt, language_rt]):\n",
    "        if len(data_points) > 0:\n",
    "            # Add some jitter to x position\n",
    "            x = np.random.normal(i+1, 0.04, size=len(data_points))\n",
    "            # Plot data points with standard black dots\n",
    "            ax.scatter(x, data_points, alpha=1, s=25, color='black', zorder=10)\n",
    "    \n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.set_ylim(0, y_max)\n",
    "    \n",
    "    if subject_id:\n",
    "        ax.set_title(f'Subject {subject_id}: Math vs. Language Reaction Time', fontsize=14, color='black')\n",
    "    else:\n",
    "        ax.set_title(f'All Subjects: Math vs. Language Reaction Time', fontsize=14, color='black')\n",
    "    \n",
    "    ax.set_ylabel('Reaction Time (s)', fontsize=16, color='black')\n",
    "    \n",
    "    stats_text = (\n",
    "        f'Math (n={len(math_rt)}):\\n'\n",
    "        f'Mean = {np.mean(math_rt):.3f}s\\n'\n",
    "        f'SD = {np.std(math_rt):.3f}s\\n\\n'\n",
    "        f'Language (n={len(language_rt)}):\\n'\n",
    "        f'Mean = {np.mean(language_rt):.3f}s\\n'\n",
    "        f'SD = {np.std(language_rt):.3f}s\\n\\n'\n",
    "        f't-test: p = {p_value:.4f}' if p_value is not None else 'Not enough data for t-test'\n",
    "    )\n",
    "    \n",
    "    ax.text(0.5, 0.98, stats_text,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='top',\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
    "            fontsize=13, zorder=15)\n",
    "    \n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.3, color='black')\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='black', labelsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure to PDF\n",
    "    pdf.savefig(fig)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'math_rt_mean': np.mean(math_rt) if len(math_rt) > 0 else 0,\n",
    "        'math_rt_std': np.std(math_rt) if len(math_rt) > 0 else 0,\n",
    "        'language_rt_mean': np.mean(language_rt) if len(language_rt) > 0 else 0,\n",
    "        'language_rt_std': np.std(language_rt) if len(language_rt) > 0 else 0,\n",
    "        'math_rt_count': len(math_rt),\n",
    "        'language_rt_count': len(language_rt),\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "# Load data for each subject\n",
    "subject_data = {}\n",
    "all_data = []\n",
    "\n",
    "print(\"Starting Math vs. Language Analysis...\")\n",
    "\n",
    "for subject_num in [2, 4, 5]:\n",
    "    file_path = os.path.join(base_path, f\"subject{subject_num}_matched.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data for Subject {subject_num}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Store data for individual subject analysis\n",
    "        subject_data[subject_num] = df\n",
    "        \n",
    "        # Add subject identifier and append to combined data\n",
    "        df['subject'] = subject_num\n",
    "        all_data.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File not found for Subject {subject_num}\")\n",
    "\n",
    "# Combine all data\n",
    "if all_data:\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Combined data has {len(combined_data)} rows\")\n",
    "else:\n",
    "    print(\"No data found for any subject\")\n",
    "    combined_data = None\n",
    "\n",
    "# Create dictionaries to store results\n",
    "accuracy_results = {}\n",
    "rt_results = {}\n",
    "\n",
    "# Analyze each subject individually\n",
    "print(\"\\n=== Analyzing Math vs. Language Question Performance ===\")\n",
    "for subject_num, data in subject_data.items():\n",
    "    print(f\"\\nAnalyzing Subject {subject_num}...\")\n",
    "    \n",
    "    # Accuracy analysis\n",
    "    print(f\"  Analyzing accuracy for Subject {subject_num}...\")\n",
    "    accuracy_results[subject_num] = plot_accuracy_comparison(data, subject_num)\n",
    "    \n",
    "    # Reaction time analysis\n",
    "    print(f\"  Analyzing reaction times for Subject {subject_num}...\")\n",
    "    rt_results[subject_num] = plot_rt_comparison(data, subject_num)\n",
    "\n",
    "# Analyze combined data\n",
    "if combined_data is not None:\n",
    "    print(\"\\nAnalyzing combined data (all subjects)...\")\n",
    "    accuracy_results['all'] = plot_accuracy_comparison(combined_data)\n",
    "    rt_results['all'] = plot_rt_comparison(combined_data)\n",
    "\n",
    "# Close the PDF file\n",
    "pdf.close()\n",
    "\n",
    "# Print summary of accuracy results\n",
    "print(\"\\nSummary of Math vs. Language Accuracy Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Subject':<10} {'Math Acc.':<10} {'Lang Acc.':<10} {'Math n':<8} {'Lang n':<8} {'Test':<15} {'p-value':<10} {'Significant?':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for subject, result in accuracy_results.items():\n",
    "    if result:\n",
    "        subject_label = \"All\" if subject == 'all' else f\"Subject {subject}\"\n",
    "        math_acc = f\"{result['math_accuracy']:.3f}\"\n",
    "        lang_acc = f\"{result['language_accuracy']:.3f}\"\n",
    "        significant = \"Yes (p<0.05)\" if result['p_value'] is not None and result['p_value'] < 0.05 else \"No\"\n",
    "        p_val = f\"{result['p_value']:.4f}\" if result['p_value'] is not None else \"N/A\"\n",
    "        \n",
    "        print(f\"{subject_label:<10} {math_acc:<10} {lang_acc:<10} {result['math_count']:<8} {result['language_count']:<8} {result['test_name']:<15} {p_val:<10} {significant:<12}\")\n",
    "\n",
    "# Print summary of reaction time results\n",
    "print(\"\\nSummary of Math vs. Language Reaction Time Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Subject':<10} {'Math RT':<15} {'Lang RT':<15} {'Math n':<8} {'Lang n':<8} {'t-stat':<10} {'p-value':<10} {'Significant?':<12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for subject, result in rt_results.items():\n",
    "    if result:\n",
    "        subject_label = \"All\" if subject == 'all' else f\"Subject {subject}\"\n",
    "        math_rt = f\"{result['math_rt_mean']:.3f} ± {result['math_rt_std']:.3f}\"\n",
    "        lang_rt = f\"{result['language_rt_mean']:.3f} ± {result['language_rt_std']:.3f}\"\n",
    "        significant = \"Yes (p<0.05)\" if result['p_value'] is not None and result['p_value'] < 0.05 else \"No\"\n",
    "        t_stat = f\"{result['t_stat']:.3f}\" if result['t_stat'] is not None else \"N/A\"\n",
    "        p_val = f\"{result['p_value']:.4f}\" if result['p_value'] is not None else \"N/A\"\n",
    "        \n",
    "        print(f\"{subject_label:<10} {math_rt:<15} {lang_rt:<15} {result['math_rt_count']:<8} {result['language_rt_count']:<8} {t_stat:<10} {p_val:<10} {significant:<12}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(f\"\\nAll plots saved to: {pdf_path}\")\n",
    "print(\"Math vs. Language Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0c2eb-c918-4ecb-8b2b-c7f746ded442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2b623-89e0-43c4-b8e4-6bc352263422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b52568-3877-4637-bbfe-e457aff59f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a6cb1b-b427-425c-a7c3-2ba8a396a684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
